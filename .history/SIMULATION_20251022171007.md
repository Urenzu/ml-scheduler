# SIMULATION.md

# Simulation Strategies for ML Scheduler Demo

This document outlines options for simulating ML workloads in the prototype scheduler/data pipeline demo.

---

## 1. Time-based Simulation (Toy Simulation)

* **Method**: Use `time.sleep(duration)` to mimic job execution.
* **Pros**:

  * Simple to implement
  * Lightweight, no real compute required
  * Useful for testing scheduler mechanics (queue, priority, workers)
* **Cons**:

  * No real CPU/GPU usage
  * Cannot simulate resource contention
  * Checkpointing and recovery not meaningful

**Example:**

```python
import time

def run_job():
    time.sleep(5)  # simulate 5-second job
```

---

## 2. Partial Simulation Using Synthetic Workloads

* **Method**: Perform real computations that mimic ML workloads.
* **Approach Options:**

  * CPU-bound: large matrix multiplications or tokenization loops
  * Mini ML jobs: small neural networks trained on synthetic data
* **Checkpointing**: Save intermediate results or model weights.

### Example 1: CPU-bound Synthetic Workload

```python
import numpy as np, os

progress_file = 'progress.npy'

# Load checkpoint if exists
if os.path.exists(progress_file):
    A = np.load(progress_file)
else:
    A = np.random.rand(5000, 5000)

# Workload loop
for i in range(5):
    A = np.dot(A, A)
    np.save(progress_file, A)  # checkpoint
```

### Example 2: Mini ML Job with Checkpointing (PyTorch)

```python
import torch, torch.nn as nn, torch.optim as optim

model = nn.Linear(10, 1)
optimizer = optim.SGD(model.parameters(), lr=0.01)
checkpoint_file = 'checkpoint.pt'

# Resume from checkpoint if exists
if os.path.exists(checkpoint_file):
    checkpoint = torch.load(checkpoint_file)
    model.load_state_dict(checkpoint['model_state'])
    optimizer.load_state_dict(checkpoint['optimizer_state'])
    start_step = checkpoint['step'] + 1
else:
    start_step = 0

# Training loop
for step in range(start_step, 5):
    x = torch.randn(32, 10)
    y = torch.randn(32, 1)
    loss = ((model(x) - y)**2).mean()
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    # Save checkpoint
    torch.save({'step': step, 'model_state': model.state_dict(), 'optimizer_state': optimizer.state_dict()}, checkpoint_file)
```

---

## 3. Full ML Jobs

* **Method**: Train actual models on real datasets.
* **Pros**:

  * Realistic resource usage
  * True checkpointing and recovery
* **Cons**:

  * Requires GPU/CPU resources
  * Long execution time
  * More complex setup for demo

---

## 4. Recommendation for Demo v1

* Use **mini ML jobs** (PyTorch/TensorFlow) or **CPU-bound synthetic workloads**.
* Implement **checkpointing** to allow job resumption.
* Scheduler can **resume interrupted jobs** from last checkpoint.
* Provides **realistic ML behavior** without heavy compute requirements.

---

**Summary:**

* Time-based sleep: easy but unrealistic
* Synthetic CPU/mini ML jobs: balanced realism, supports checkpointing
* Full ML jobs: most realistic but heavy

Checkpointing is strongly recommended even for demos to mimic recovery in real ML workflows.
