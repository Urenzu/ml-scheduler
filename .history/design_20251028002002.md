# DESIGN.md

## Project Overview

This project implements a **prototype ML compute scheduling system** integrated with a **data pipeline for research datasets**. It supports:

- Ingestion of streaming or batch data  
- Storage in **PostgreSQL** (metadata) and **Parquet** (columnar datasets)  
- Distributed ML workload scheduling  
- Simulation of ML training jobs with checkpointing  
- Monitoring, reproducibility, and scalable execution  

**Goal:** Provide a full-stack ML infrastructure playground to learn **data storage**, **pipeline processing**, and **scheduler design**.

---

## Architecture

### 1. Data Ingestion Layer

- **Sources:** CSV, JSON, or simulated streaming data  
- **Tools:** Kafka (streaming), Python ingestion scripts  
- **Destination:** Raw data written as **Parquet files** with metadata tracked in **PostgreSQL**

### 2. Data Storage & Processing Layer

- **Raw:** Unprocessed Parquet data  
- **Processed:** Deduplicated, cleaned datasets  
- **Curated:** Ready-for-training datasets with enforced schema  
- **Tools:** PyArrow, Pandas, optional Spark for scalable transformations  
- **Metadata:** PostgreSQL maintains dataset lineage, schema, and version info  

### 3. Compute Scheduler

- Lightweight Python scheduler managing ML jobs via **priority queues**  
- **Scheduling algorithms:**  
  - FIFO  
  - Gang Scheduling (for distributed / multi-GPU jobs)  
  - Dominant Resource Fairness (DRF)  
  - Backfilling (for efficient utilization)  
- Persistent queue options: **Redis** or **PostgreSQL**  
- Supports retry, preemption, and checkpoint resumption  

### 4. Worker / Execution Layer

- Workers continuously poll scheduler queues for pending jobs  
- Executes **ML simulations**:
  - CPU-bound matrix ops or tokenization loops  
  - Mini PyTorch/TensorFlow training with checkpointing  
- Reports progress, logs, and metrics back to PostgreSQL  

### 5. Monitoring & Observability

- Real-time tracking of:
  - Job status, resource utilization, queue depth  
  - Dataset versions and model checkpoints  
- **Tools:**  
  - Prometheus + Grafana → system metrics  
  - MLflow → experiment tracking and reproducibility  

---

## Workflow

1. **Data Ingestion:**  
   Kafka / Python producer feeds raw data → Parquet files + metadata stored in PostgreSQL  

2. **Data Processing:**  
   Spark / Pandas transformations clean, deduplicate, and tokenize data → processed & curated Parquet layers  

3. **Job Scheduling:**  
   User submits ML jobs via CLI or API → Scheduler selects resources using DRF, gang scheduling, etc.  

4. **Execution:**  
   Workers pick up jobs → simulate or run actual ML training → checkpoint progress  

5. **Monitoring:**  
   Prometheus + MLflow visualize metrics, logs, and job history  

---

## File Structure

ml_scheduling/
├── data.py # Dataset handling, Parquet + PostgreSQL integration
├── ingestion.py # Simulated streaming / batch ingestion
├── processing.py # Deduplication, cleaning, transformation
├── scheduler.py # Job queue, scheduling algorithms (FIFO, DRF, etc.)
├── worker.py # Worker process executing ML jobs
├── simulation.py # ML workload simulation / checkpointing
└── main.py # Orchestration: ingestion → processing → scheduling → execution


### File Descriptions

- **data.py** – Defines Parquet I/O utilities and metadata sync with PostgreSQL  
- **ingestion.py** – Handles Kafka/batch ingestion simulation  
- **processing.py** – Deduplication and transformation logic  
- **scheduler.py** – Implements scheduling algorithms and resource management  
- **worker.py** – Job polling and execution logic  
- **simulation.py** – Lightweight synthetic or ML-based workloads  
- **main.py** – End-to-end orchestration script  

---

## Design Considerations

- **Scalability:**  
  Multi-node, multi-GPU scheduling with gang scheduling or DRF  

- **Reproducibility:**  
  Dataset version tracking + MLflow logging  

- **Fault Tolerance:**  
  Checkpointing, retries, and recovery on worker failure  

- **Data Efficiency:**  
  Parquet columnar storage, compression, and partitioning  

- **Extensibility:**  
  Modular components → new schedulers, models, or data sources can be added easily  

---

## Tech Stack

| Layer          | Tools / Frameworks                  |
| -------------- | ---------------------------------- |
| Ingestion      | Kafka, Python scripts               |
| Storage        | PostgreSQL (metadata), Parquet (data) |
| Processing     | PyArrow, Pandas, Spark (optional)  |
| Scheduler      | Python queues, Redis/PostgreSQL     |
| Worker/ML      | PyTorch/TensorFlow (simulations)   |
| Monitoring     | MLflow, Prometheus, Grafana        |

---

## Summary

This system integrates **data ingestion**, **Parquet-based storage**, **PostgreSQL metadata**, **ML workload scheduling**, and **training simulation** into a cohesive, modular prototype.

It serves as an educational and experimental platform to explore:

- Data engineering (Parquet, metadata, ingestion)  
- ML infrastructure (scheduling, resource allocation)  
- Observability (metrics, reproducibility, fault tolerance)  

---
