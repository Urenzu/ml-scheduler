# ML Compute Scheduling + LLM Data Pipeline Design

## Overview

This project implements a **prototype ML compute scheduling system** integrated with a **data lake pipeline** for LLM-style research datasets. The goal is to provide a fully functional system that handles data ingestion, preprocessing, storage, and distributed training scheduling, while supporting monitoring, reproducibility, and scalability.

---

## Architecture

### Components

1. **Data Ingestion Layer**

   * **Source**: Simulated streaming or batch datasets (CSV, JSON).
   * **Tools**: Kafka for streaming ingestion.
   * **Function**: Collect raw data and push to raw storage layer.

2. **Data Lake Storage**

   * **Raw Layer**: Unprocessed ingestion data (S3 / local storage).
   * **Processed Layer**: Deduplicated, cleaned, and tokenized Parquet/Delta Lake files.
   * **Curated Layer**: Ready-for-training datasets with schema enforcement.
   * **Tools**: PyArrow, Pandas, Spark, Delta Lake.

3. **Data Processing / Transformation Layer**

   * **Functions**: Deduplication, quality checks, feature extraction.
   * **Frameworks**: Spark for distributed processing, dbt for transformations.
   * **Output**: Parquet/Delta Lake datasets.

4. **Compute Scheduler / Orchestrator**

   * **Job Queue**: Manages ML tasks and priorities.
   * **Scheduler Core**: Allocates GPU/CPU resources and schedules jobs.
   * **Resource Manager**: Tracks node availability and utilization.
   * **Execution**: Runs distributed training jobs via Ray or PyTorch Distributed.
   * **Strategies**: FIFO, priority-based, gang scheduling, backfilling.

5. **Monitoring & Observability**

   * Track GPU/CPU usage, job status, data processing metrics.
   * **Tools**: Prometheus, Grafana, MLflow.

6. **Web / API Layer (Optional)**

   * Job submission and monitoring interface.
   * REST API to submit training tasks, query job status, and fetch logs.

---

## Workflow

1. **Data Ingestion**

   * Simulated or real streaming data arrives.
   * Kafka produces messages to raw storage.

2. **Data Processing**

   * Batch Spark jobs pick up raw data.
   * Deduplicate, validate, and transform data.
   * Store in processed and curated layers as Parquet/Delta Lake.

3. **Job Scheduling**

   * Users submit ML training jobs via CLI/API.
   * Scheduler assigns resources based on job size, priority, and data locality.
   * Tasks execute on available GPU nodes.

4. **Monitoring & Logging**

   * All tasks emit logs and metrics.
   * Scheduler monitors job status, retries failed tasks, and triggers alerts.

5. **Traceability**

   * All dataset versions, preprocessing steps, and model checkpoints are tracked.
   * Supports reproducibility and debugging.

---

## Technologies & Tools

| Layer          | Tools / Frameworks                 |
| -------------- | ---------------------------------- |
| Data Ingestion | Kafka, Python producer/consumer    |
| Storage        | S3 / local FS, Parquet, Delta Lake |
| Processing     | PyArrow, Pandas, Spark, dbt        |
| Scheduler      | Ray, Kubernetes, Python queues     |
| Monitoring     | Prometheus, Grafana, MLflow        |
| Job API/UI     | FastAPI / Flask (optional)         |

---

## Design Considerations

* **Scalability**: Scheduler must handle multiple distributed ML tasks; use gang scheduling and backfilling.
* **Fault Tolerance**: Jobs must retry on failure; checkpoint models and datasets.
* **Data Efficiency**: Columnar storage (Parquet/Delta) to minimize I/O; compression for storage savings.
* **Reproducibility**: Track dataset versions, preprocessing scripts, and job configurations.
* **Resource Utilization**: Scheduler maximizes GPU/CPU usage; monitor memory and network.
* **Extensibility**: Modular design allows adding new ML models, data transformations, or storage layers.

---

## Project Roadmap

1. **Setup Data Lake**

   * Local folders / S3 buckets for raw, processed, curated layers.
   * Implement basic Parquet write/read scripts.

2. **Implement Data Ingestion**

   * Simulate streaming data via Kafka.
   * Write ingestion scripts to raw storage.

3. **Build Processing Pipelines**

   * Spark jobs for deduplication and quality checks.
   * Transform and store curated datasets in Parquet/Delta Lake.

4. **Develop Compute Scheduler**

   * Python-based scheduler using queues.
   * Integrate with Ray or Kubernetes for distributed execution.
   * Implement scheduling strategies (FIFO, priority, gang scheduling).

5. **Integrate Monitoring**

   * Add Prometheus metrics for job and node utilization.
   * Dashboard in Grafana for observability.

6. **End-to-End Testing**

   * Submit mock ML jobs with processed datasets.
   * Verify scheduling, execution, and monitoring.

7. **Optional Enhancements**

   * Web UI / API for job submission and monitoring.
   * Integrate Terraform for infrastructure provisioning.
   * Add Delta Lake time travel for dataset versioning.

---

## References

* [Apache Parquet](https://parquet.apache.org/)
* [Delta Lake](https://delta.io/)
* [Ray Distributed Framework](https://www.ray.io/)
* [Apache Spark](https://spark.apache.org/)
* *Designing Data-Intensive Applications* by Martin Kleppmann
* [MLflow](https://mlflow.org/) for experiment tracking
