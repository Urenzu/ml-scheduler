# ML Job Scheduling Guide

## Overview

This document outlines **scheduling algorithms for ML workloads**, considerations for choosing an algorithm, and best practices for distributed ML compute scheduling. It provides a reference to design a high-throughput, fair, and scalable ML job scheduler.

---

## 1. Scheduling Algorithms

### 1.1 Weighted Load Balancing

* **Description**: Jobs are distributed across queues based on priority and queue depth.
* **Pros**: Simple to implement, fast.
* **Cons**: Ignores multi-resource constraints, gang scheduling, or job dependencies.
* **Use case**: Small-scale, homogeneous jobs.

### 1.2 Gang Scheduling

* **Description**: All tasks of a distributed ML job are scheduled simultaneously.
* **Pros**: Essential for multi-GPU/ML distributed jobs.
* **Cons**: Can delay scheduling if sufficient resources are fragmented.
* **Use case**: Multi-node ML training, Horovod, PyTorch DDP.

### 1.3 Dominant Resource Fairness (DRF)

* **Description**: Fair allocation across multiple resources (CPU, GPU, memory) by dominant share.
* **Pros**: Prevents resource starvation, handles heterogeneous workloads.
* **Cons**: More complex to implement than weighted load balancing.
* **Use case**: Multi-resource ML clusters.

### 1.4 Dynamic Priority / Aging

* **Description**: Job priority increases with waiting time to prevent starvation.
* **Formula**: `effective_priority = base_priority + alpha * waiting_time`
* **Pros**: Ensures fairness over time.
* **Use case**: Mixed workloads with varying priority.

### 1.5 Backfilling

* **Description**: Schedule smaller jobs in idle resources while waiting for larger jobs.
* **Pros**: Improves resource utilization.
* **Use case**: Mixed-size ML job clusters.

### 1.6 Hierarchical / Multi-Level Scheduling

* **Description**: Global scheduler allocates resources; local schedulers assign tasks per node.
* **Pros**: Scales to large clusters, supports heterogeneous hardware.
* **Cons**: Requires cluster-wide state management.
* **Use case**: Large-scale distributed ML research platforms.

### 1.7 Predictive / ML-Based Scheduling

* **Description**: Use historical metrics to predict job runtime and resource usage; optimize scheduling.
* **Pros**: Adapts to cluster dynamics.
* **Cons**: Requires job history and ML models.
* **Use case**: High-throughput clusters where jobs vary in runtime and resources.

---

## 2. Choosing the Right Algorithm

1. **Job Type**:

   * Single-node, short jobs → weighted load balancing.
   * Distributed multi-GPU jobs → gang scheduling.

2. **Resource Constraints**:

   * Heterogeneous resources → DRF.
   * Multi-resource jobs (GPU + memory + CPU) → DRF or predictive scheduling.

3. **Cluster Size**:

   * Small cluster → simple algorithms (weighted, FIFO).
   * Large cluster → hierarchical/multi-level or DRF.

4. **Fairness vs Throughput**:

   * Prioritize fairness → DRF + dynamic priority.
   * Prioritize throughput → backfilling + predictive scheduling.

5. **Dynamic Workloads**:

   * Jobs with unpredictable runtime → predictive or ML-based scheduling.

---

## 3. Recommended Strategy for ML Clusters

For a distributed ML platform:

1. **Gang Scheduling** → ensure all tasks of multi-node jobs run together.
2. **Dominant Resource Fairness (DRF)** → fair allocation of CPU/GPU/memory.
3. **Backfilling** → fill idle resources with smaller jobs.
4. **Dynamic Priority / Aging** → prevent starvation.
5. **Optional Predictive Scheduling** → optimize job completion and resource usage using historical metrics.

---

## 4. Implementation Notes

* **Data Structures**:

  * Priority queues or heaps for pending jobs.
  * Cluster state map (nodes × resources).
  * Job metadata: resources, estimated duration, priority, dependencies.

* **Persistent Queues**:

  * Redis sorted sets for atomic operations and distributed persistence.

* **Monitoring & Metrics**:

  * Track job runtimes, resource utilization, queue depth.
  * Use metrics for backfilling decisions and predictive scheduling.

* **Integration**:

  * Combine with ML data pipelines (data lake / Parquet) for end-to-end scheduling.
  * Scheduler should handle retries, checkpointing, and logging for reproducibility.

---

## References

* Dominant Resource Fairness paper: Ghodsi et al., 2011
* Gang scheduling for distributed ML: [Ray documentation](https://docs.ray.io)
* Backfilling strategies: HPC scheduling literature
* Kubernetes scheduler: Multi-resource and priority-aware scheduling
